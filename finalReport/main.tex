\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Larp - Laplace Additive Regression Trees}


\author{Peter Rindal 
\And
Trung Viet Vu 
\And
Hung Viet Le}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}

Boosting is a successful machine learning algorithm for a number of regression and classification tasks. Combined with tree base learner, boosted ensemble algorithms are shown to deliver highly accurate and strong predictive models. In this work, we first study a well-known approach called Multiple Additive Regression Trees (MART). By looking at its weakness, we proposed a different approach named Laplace Additive Regression Trees (LART) to mitigate this by injecting random Laplace noises. In the experiment, we compare the performance of LART and MART on the dataset that has been used in previous papers. Our results is promising as the proposed method yields significant gains in accuracy in this particular task.

\end{abstract}

\section{Introduction}

% introduce the problem. See the dart paper for a good intro. We want to frame it as the MART algorithm suffers from later trees not contributing much and that the consecutive trees have too much in common. i.e. the same predicates are often chosen between consecutive tree when the learning rate/shrinkage is small , e.g. 0.05. 

Ensemble modeling is a powerful way to improve the performance of a single learning model.  It can be able to out-perform any single classifier within the ensemble \cite{Dietterich2000}. Many methods for constructing ensembles have been developed. One popular approach is Bagging and Random Forest \cite{Breiman2001}, in which different subsets of the training examples are used to learn independent predictors. Another practical technique is boosting. Boosted algorithms focus on improving the current model by iteratively adjusting the learning algorithm between iterations. In general, this method often results in better reducing the bias error and achieving strong predictive models. \\

In 2001, Friedmn introduced an ensemble of boosted regression trees - MART \cite{mart}, which combines boosting with tree base learner. At each iteration a tree is added to fit the gradient using least square. The resulting model forms an ensemble of weak predictive models that gives highly accurate predictions. Later on, Caruana and Niculescu-Mizail \cite{Caruana06anempirical} demonstrated the application of MART to build highly successful models for many learning tasks. Despite its success, this algorithm still suffers from the problem of over-fitting where the learned model does not generalize well to unseen data. This issue is addressed in \cite{VinayakG15} as \textit{over-specialization}: trees added at later iterations tend to have too much in common, and add negligible contribution towards the prediction of all the remaining instances. In real-world settings, the MART algorithm often starts with a single tree that learns the bias of the problem while the additive trees in the ensemble learn the deviation from this bias. In this sense, the ensemble is sensitive to the decision made by the first tree.\\

One commonly used tool to approach the problem of over-specialization in MART is \textit{shrinkage} \cite{mart}. Empirically this regularization technique reduces the impact of each tree by a constant value - \textit{shrinkage factor}, which helps dramatically improve the model's generalization ability. However, while the over-fitting problem can be mitigated by shrinkage, the fundamental issue of over-specialization still remains, i.e. as the size of the ensemble increases, the contribution of the later trees is negligible \cite{VinayakG15}. Motivated by this, we hypothesize that randomizing the process of selecting predicates can provide another effective regularization for MART and propose the LART algorithm.


\subsection{Mart - Random Forest Hybrid}

% maybe one of you can write this. Talk about how our randomization technique allows a smooth transition between a random forest technique and the original MART. This can be tuned and talk about how the use of the laplace random variable effects this randomization. I.e. if one predicate/split is significantly better, then it will still be pick with good probability (assuming a moderate amount of noise.)

In this work, we explore an improvement in MART to address the issue of over-specialization. Specifically, we propose a method called Laplace Additive Regression Trees (LART), which injects randomness into the learning algorithm.
By adding Laplace noises to the calculation of the loss function, a predicate can be sampled in a way such that a greater reduce in the loss function results in it being exponentially more likely to be selected. Instead of selecting the predicate that minimizes the loss function, we select the predicate that minimizes the sum of the loss function and the Laplace noise. Predicates chosen in this way are more uncorrelated and hence the variance in the learners increases.\\

Our motivation for this approach is a hybrid view between MART and Random Forest. While MART takes the advantage of boosting algorithms in achieving low bias and low variance, it can be sensitive to noise and outliers as mentioned above. On the other hand, Random Forest benefits from the diversity among the learned classifiers and hence it is more robust to noise and outliers. However, it has the major disadvantage of regression trees, that is, their inaccuracy. Our proposed method tries to capture the advantage of these two approaches with appropriate additive Laplace noise. When the Laplace distribution is scaled by too much, the noise will drown out the contribution that the loss function may have. Predicates at each iteration are likely to be chosen randomly. This leads to special case wherein LART is similar to a Random Forest algorithm. Conversely, if the Laplace noise is too small, the loss function will dominate and effectively result in the MART algorithm. Tuning the scale of Laplace noise thus is a major point of investigation for our work.



\subsection{Differential Privacy }

peter can write this

\section{Related Work}

Hung, can you do this section?

\section{Over Specialization}

cite the dart paper and talk about the same thing...

\section{The Larp Algorithm}

introduce the algorithm formally. Its simply Mart with noise added to the splits loss function.

\section{Evaluation}

\section{Conclusion}
In this project, we first review the approach of using Multiple Additive Regression Trees (MART) to regression and classification tasks. The weakness of MART is trees added at later iterations have significantly diminishing contributions. By injecting some randomness to generating ensembles classifiers, we propose a different approach, called LART, to provide efficient regularization for MART. Our experiment shows that LART out-performs MART when the number of trees increases considerably. This strengthen our hypothesis that LART is more robust to over-specialization.\\

This study also suggest some point of investigation. One direction is tuning the scale parameter along with other traditional parameters in MART and examining their interaction. Additionally, we might evaluate the performance of LART on more complicated datasets and other machine learning tasks, as in data mining.



\nocite{*}
\bibliographystyle{plain}
\bibliography{ref}


\end{document}
