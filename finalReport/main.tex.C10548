\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref, dsfont}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Larp - Laplace Noise and Multiple Additive Regression Trees}


\author{Peter Rindal 
\And
Trung Viet Vu 
\And
Hung Viet Le}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

 
\maketitle

\begin{abstract}

TODO

\end{abstract}

\section{Introduction}

introduce the problem. See the dart paper for a good intro. We want to frame it as the MART algorithm suffers from later trees not contributing much and that the consecutive trees have too much in common. i.e. the same predicates are often chosen between consecutive tree when the learning rate/shrinkage is small , e.g. 0.05. 
 
  

\subsection{Mart - Random Forest Hybrid}



maybe one of you can write this. Talk about how our randomization technique allows a smooth transition between a random forest technique and the original MART. This can be tuned and talk about how the use of the laplace random variable effects this randomization. I.e. if one predicate/split is significantly better, then it will still be pick with good probability (assuming a moderate amount of noise.)




\subsection{Differential Privacy }

An alternative way to interpret this work is from the perspective of differential privacy. The general setting of differential privacy is that we wish to run an algorithm on some sensitive dataset and reveal the result. Moreover, this result must have privacy guarantees with respect to how much information on \emph{any single record} can be obtained from the result alone. 

While this work is not concerned about privacy, we are concerned with the problem of over fitting which decision trees are particularly susceptible to. Overfitting is a general problem in machine learning where the model learns random errors in the training set as opposed to the underlying distribution. 

In more detail, differential privacy guarantees that for two datasets $X,Y$ where $Y$ has one training example $X$ does not, then the probability that the training algorithm on $X$ and $Y$ will produce the same models should be bounded by
$$
	\forall m \in\mbox{Models} \ : \ \Pr[m = \textsc{Train}(X)] \leq \exp(\epsilon) \Pr[m= \textsc{Train}(Y)]
$$
What this definition says is that the algorithm \textsc{Train} should output the model $m$ with roughly the same probability when the parameter $\epsilon\in (0,\infty]$ is near $0$. In the case that $\epsilon$ is very large, this definition of differential privacy places no constraint the the output of the algorithm. In the setting of differential privacy, a smaller value for $\epsilon$ is considered more private, e.g. $\epsilon=0.1$. 

Comparing this to the problem of overfitting, we can see many parallels. That is, if $Y = X \cup {y}$ and $y$ is an training example with which significantly deviates from the underlying distribution, then a training algorithm which is differentially privacy will give guarantees about the impact that this outlier can have. This observation is the primary inspiration of this for this work. In particular, we do not try to fully satisfy the requirements of differential privacy, instead we apply differentially private techniques in a heuristically manner.

\section{Related Work}    
 
Hung, can you do this section?

\section{Over Specialization}

cite the dart paper and talk about the same thing...

\section{The Algorithm}

\subsection{Mart}
We begin by describing the Mart algorithm on which we will build the Lart algorithm. The Mart algorithm can be viewed as a (functional) gradient decent algorithm \cite{friedman2001}. The model is then constructed iteratively by generating a series of trees, each correcting the previous set. This is done by taking the derivative of the loss function based the current predictions and updates the model by adding another regression tree that is trained to fit the negation of these derivative. 

The input to the algorithm is a labeled dataset $D=\{(x,y)\}$ where $x\in \mathcal{X}$ is a vector in the feature space $\mathcal{X}$ and $y\in \mathcal{Y}$ is the labels in prediction space. In addition to $D$, the algorithm takes as input a loss function $\mathcal{L}_{(x,y)} :  \mathcal{Y} \mapsto \mathds{R} $ which the models prediction to a real value denoting the error of this prediction. In our case, we are performing regression and chose to use the $L2$ loss function and therefore we have $\mathcal{L}_{(x,y)}(\hat{y} ) = (y- \hat{y})^2$.
 
 
The algorithm begins by initializing the current model $M:\mathcal{X}\mapsto \mathcal{Y}$ to some default map. Let $M(x)$ denote the prediction of the model at the feature vector $x$ and $\mathcal{L}'_{(x,y)}(M(x))$ denote the derivative of the loss function at $M(x)$. At each iteration, Mart then creates new dataset datasets $D'=\{(x,-\mathcal{L}'(M(x)))\}$  and learns a new tree model $T$ for $D'$. This tree $T$ is trained to predict the negation of the the derivate of the loss function. The current model is then updated as $M(x):= M(x) + T(x)$, thereby reducing the loss.


An individual tree $T$ is trained in a relatively straightforward way. It is assumed that there exists a set of predicates each of which $P:\mathcal{X} \mapsto \{0,1\}$ map a feature vector to a single bit.  Initialized, all records in the training set $D'$ are mapped to the root of the tree. The tree is then recursively constructed by selecting a lead node $L$. For each predicate $P_i$ the recodes at the current node $(x,y)\in L $ are mapped to $L$'s newly created children $L_{i,0},L_{i,1}$, where $(x,y)$ is mapped to $L_{i,P_i(x)}$.

In the case of the $L2$ loss function describe above, its derivative is $\mathcal{L}'_{(x,y)}(\hat{y}) = 2(y - \hat{y})$. This will be the loss function of choice in the evaluation section as it is suitable to the regression task in question. However, other loss functions can be defines for different tasks such as classification. In this case, the  the logistic loss function can be used $\mathcal{L}_{(x,y)} (\hat{y}) =( 1 + \exp(\lambda y \hat{y}))^{-1}$ and consecutive models will be trained to fit its derivative.



\subsection{Lart}
We now turn our attention to the Lart technique discussed in this paper. One problem with the Mart algorithm is that of over specialization as discussed above\cite{dart}. In this case the later trees contribute very little to the  model as trees start to fail to fit the derivative of the loss function. The related work Dart \cite{dart} suggest that randomizing how these trees are constructed can provide some benefit in that later trees continue to improve the model. While they put forth a technique based on dropout, our technique is based on methods from differential privacy.

In more detail, we randomize the construction of 

\section{Evaluation}

We evaluate the our Lart technique by comparing it to the Mart, random forests, and Dart\cite{dart} algorithms. We implemented each of their algorithms within our own framework. While Dart\cite{dart} also implemented implemented these algorithms, the numbers we get are slightly different than what they report. 

For the dataset, we choose to use the CT slice dataset from \cite{graf20112d} which can be found at \cite{uci_ctSlice}. This datset contains 53500 histograms created from CT scans of 74 individuals. The task is to locate the position that the scan was taken. Each image is represented as 386 features. We used 10-fold cross validation to compare our algorithm. As with the prior work that we compare to \cite{dart}, we ensure that the splits are chosen such that each individual is completely in the test or training datasets.


\begin{figure}	\centering
	\begin{tabular}{|r|}
 \hline
 
 \hline
               RF \\
               MART \\
               Lart \\\hline
	\end{tabular} 	
	\caption{ }
\end{figure}



Table \ref{tab:compare} show the performance of the Larp technique as compared Mart, Dart, and random forest.

\section{Conclusion}
\bibliography{document}{}
\bibliographystyle{plain}
\end{document}
